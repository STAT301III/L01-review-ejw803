---
title: "L01 Review"
subtitle: "Data Science III (STAT 301-3)"
author: "Emily Wang"
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
---

## Overview

The goal of this lab is to review vocabulary and concepts from the Data Science II (STAT 301-2).

## Questions

Please complete the following questions. Be sure that your solutions are clearly indicated and that your document is neatly formatted.

### Question 1

Provide a brief outline/overview of the steps involved in a supervised machine learning process. Could provide this as a bulleted list.

1.  Perform a preliminary data quality check (making sure names are correct, variable types are processed correctly) and look at outcome variable before splitting data
2.  Based on the distribution of outcome variable & amount of data, split data
3.  Resample data (cross validation or validation set)
4.  Perform a brief EDA either on the training data or, if there are enough observations, reserve part of the dataset for EDA only or slice a part of the training data for EDA only
5.  Based on EDA, create a recipe for feature engineering (then prep & bake to check that data processed as expected)
6.  Set up models and tune relevant parameters through grid search
7.  Determine best parameters based on performance after fitting to all training data
8.  Compare model performance based on selected metrics
9.  Finalize the best model with the best parameters and fit to testing
10. Evaluate model performance on testing data based on selected metrics

<br>

### Question 2

Explain the difference between supervised and unsupervised learning.

Supervised learning revolves around a chosen outcome variable, such as predictive models. In supervised learning, we aim to figure out how the predictors and the outcome variable are related. Meanwhile, unsupervised learning has no identified outcome. Instead, we aim to look at patterns between predictors.

<br>

### Question 3

In general, we can classify a model by its purpose into 1 of the 3 categories below. Provide a brief description of the goals of these model classes.

1.  Descriptive Models: **the goal is to describe the data through important characteristics**. We want to understand more of the data andidentify relevant patterns of interest.

2.  Inferential Models: **the goal is to test hypotheses about the data**. We want to know more about relationships between the predictors and the outcome, or between predictors.

3.  Predictive Models: **the goal is to make an estimation based on existing data.** We want to obtain the most accurate predictions of the outcome variable based on the predictor variables.

<br>

### Question 4

We can describe predictive models by how they were developed as being either mechanistic or empirically driven.

a.  What does it mean to be a mechanistic model?

A **mechanistic model** is dependent upon assumptions about the data. For example, I want to predict how much wine I will get from a season based on the amount of grapes harvested, weather forecasts, sugar level, acidity level. This mechanistic model is based on assumptions of how the wine is produced, harvested, and developed.

b.  What does it mean to be an empirically driven model?

An **empirically driven model** is based on existing data, without as many theoretical or probabilistic assumptions. For example, I want to predict how much wine I will get from a season based on past harvests. This empirically driven model is based on historical data.

c.  How does the mechanistic and empirically driven model terminology relate to the parametric and nonparametric model terminology?

Mechanistic model terminology is related parametric terminology because the parametric model relies on a specified underlying distribution, like the mechanistic model relies on certain assumptions about the dataset. Meanwhile, empirically driven model terminology is more related to nonparametric model terminology, which do not rely on assumptions about the distribution like the empirically driven model does not rely on assumptions about the data.

d.  In general, is a mechanistic or empirically driven model easier to understand? Explain.

I would say a mechanistic model is easier to understand because we base the model equations off of the characteristics and processes we observe, whereas an empirically driven model is more opaque because we do not know what drives the model and what assumptions are made.

e.  How does mechanistic and empirically driven model terminology relate to the idea of model flexibility? That is, which would be more or less flexible than the other.

Empirically driven models are more flexible because the model parameters can be changed and they are not bound by the underlying assumptions.

e.  Describe the bias-variance trade-off when considering the use of a mechanistic or empirically driven model.

More flexible models (like empirically driven models) lead to higher variance and lower bias. Bias can lead to higher errors due to oversimplification of the data, which can be an issue for mechanistic models that are based on assumptions. Variance is more of a problem for empirically driven models, which can be too flexible and lead to overfitting.

<br>

### Question 5

Explain the difference between a regression and classification machine learning (ML) problems.

Regression problems are used for a continuous outcome, whereas classification problems are used for categorical outcomes.

<br>

### Question 6

When splitting the data, why is it useful to stratify by the outcome/target variable?

Stratifying the data is useful when the outcome variable is unequally distributed. Stratifying ensures that the testing and training have a similar distribution of the outcome variable. Without stratifying, one of the sets may get a disproportionate amount of a certain outcome, whereas the other set may have nothing, which would detrimentally affect model performance.

<br>

### Question 7

Briefly describe how v-fold cross validation with repeats is used to estimate test RMSE. Also provide an explanation of why we use it.

V-fold cross validation starts with V-fold cross validation, which resamples the data into multiple training/testing sets to produce multiple models and multiple performance statistics. Repeating this process of V-fold cross validation a number of times reduces noise and dramatically decreases RMSE because we average each of the V replicates. Because we get V folds \* R repeats models, averaging more statistics improves the model accuracy.

<br>

### Question 8

Briefly describe model tuning and why we use it.

Model tuning describes the process in which we optimize the model parameters most commonly through a grid search, where we test multiple random tuning parameters to find the best values. When we tune more parameters, we get more models to test. We use model tuning to maximize performance and decrease the possibility of overfitting.

<br>

### Question 9

What are two common performance metrics when dealing with a regression ML problem?

-   R-squared, which measures correlation

-   RMSE, which measures accuracy

What are two common performance metrics when dealing with a classification ML problem?

-   Accuracy, which calculates the overall percentage of correct classifications

-   Area under the ROC curve, which is a numerical measurement of how good the model is at making classifications

<br>

### Question 10

A political candidate's campaign has detailed voter history data for their constituents. The campaign is interested in two questions:

1.  Given a voter's profile/data how likely is it that they will vote in favor of the candidate?

2.  How would a voter's likelihood of support for the candidate change if they had personal contact with the candidate?

Classify each question/problem as either prediction or inferential. Explain your reasoning for each.

1.  The first problem is **prediction** because we are predicting a future outcome based on historical behavior.
2.  The second problem is **inferential** because we want to know the relationship between the outcome (voter support) and a different variable (personal contact).

<br>

## Github Repo Link

<https://github.com/STAT301III/L01-review-ejw803/>
